{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/meriembenchaaben/IFT6289_Project/blob/main/script.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U accelerate\n",
        "!pip install -q -U optimum\n",
        "!pip install -q -U transformers\n",
        "!pip install -q -U auto-gptq"
      ],
      "metadata": {
        "id": "etrl-F-Y3S03",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98e8e41c-e345-4736-fb23-56e8ff129ff3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.0/9.0 MB\u001b[0m \u001b[31m30.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "id": "8lviD1GRPr-q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b7cd8443-ff6f-411a-98cc-81c9193bf184"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "with open('gdrive/My Drive/IFT6289/IFT6289_Project/data/DSLs.json', 'r') as file:\n",
        "    ontology = json.load(file)"
      ],
      "metadata": {
        "id": "D71fkFz0PgIM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LMaVJjHb2hpY",
        "outputId": "87182853-9b51-4125-a32e-11ad8fb5ffed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:4371: FutureWarning: `_is_quantized_training_enabled` is going to be deprecated in transformers 4.39.0. Please use `model.hf_quantizer.is_trainable` instead\n",
            "  warnings.warn(\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "model_name_or_path = \"TheBloke/CapybaraHermes-2.5-Mistral-7B-GPTQ\"\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name_or_path,\n",
        "                                              device_map=\"auto\",\n",
        "                                              trust_remote_code=False,\n",
        "                                              revision=\"main\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_metamodel_elements(diagram_name, data):\n",
        "\n",
        "    # Iterate through each category of designs\n",
        "    for category in data['DesignFormalisms'].values():\n",
        "        # Search in each design form\n",
        "        for design in category:\n",
        "            if design['Name'] == diagram_name:\n",
        "                return design['MetamodelElements']\n",
        "\n",
        "    return \"Diagram not found.\""
      ],
      "metadata": {
        "id": "MxNp1ZaQn1yc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generate different missing Elements"
      ],
      "metadata": {
        "id": "GhZK9cFj170v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_prompt_for_formalism(formalism_name, current_model, metamodel_elements, user_elements=None, desired_functionality=None, specific_constraints=None):\n",
        "    \"\"\"\n",
        "    Generates a prompt for the LLM based on the metamodel elements of a given formalism,\n",
        "    the current model, and optionally, desired functionality and constraints.\n",
        "\n",
        "    :param formalism_name: The name of the formalism to generate the prompt for.\n",
        "    :param current_model: A string representing the current state of the model under construction.\n",
        "    :param desired_functionality: (Optional) A string describing the desired functionality to be added.\n",
        "    :param specific_constraints: (Optional) A string describing any specific constraints that must be adhered to.\n",
        "    :return: A prompt string for the LLM.\n",
        "    \"\"\"\n",
        "    # Navigate the ontology to find the metamodel for the given formalism\n",
        "\n",
        "\n",
        "    # Construct the prompt using the current model and metamodel elements\n",
        "    if metamodel_elements:\n",
        "\n",
        "        # User specifies which metamodel elements to modify\n",
        "        if user_elements:\n",
        "          prompt_parts = [\n",
        "            f\"I am developing a {formalism_name} which currently includes: {current_model}.\",\n",
        "            f\"Suggest new elements for each of the metamodel element in {user_elements},\",\n",
        "            f\"that can complete the current model.\"\n",
        "        ]\n",
        "        # If not, pass all metamodel elements\n",
        "        else:\n",
        "          prompt_parts = [\n",
        "            f\"I am developing a {formalism_name} which currently includes: {current_model}.\",\n",
        "            f\"Suggest new elements for each of the metamodel element in {metamodel_elements},\",\n",
        "            f\"that can complete the current model.\"\n",
        "        ]\n",
        "\n",
        "        # If desired functionality is provided, add it to the prompt\n",
        "        if desired_functionality:\n",
        "            prompt_parts.append(f\"The model needs to support: {desired_functionality}.\")\n",
        "\n",
        "        # If specific constraints are provided, add them to the prompt\n",
        "        if specific_constraints:\n",
        "            prompt_parts.append(f\"This should adhere to the following constraints: {specific_constraints}.\")\n",
        "\n",
        "\n",
        "        # Specification to the LLM\n",
        "        prompt_parts.append(f\"Elements should be organized in a JSON format where key value is the name of the metamodel element.\")\n",
        "        prompt_parts.append(f\"Provide only the JSON file. Do not provide extra explanations.\")\n",
        "\n",
        "\n",
        "        prompt = \" \".join(prompt_parts)\n",
        "\n",
        "        return prompt\n",
        "    else:\n",
        "        return f\"Formalism '{formalism_name}' not found in the ontology.\"\n",
        "\n"
      ],
      "metadata": {
        "id": "nNx3BJFvO2pw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(tokenizer, model, prompt, max_new_tokens=512):\n",
        "\n",
        "    input_ids = tokenizer(prompt, return_tensors='pt').input_ids.cuda()\n",
        "    output = model.generate(inputs=input_ids, temperature=0.7, do_sample=True, top_p=0.95, top_k=40, max_new_tokens=max_new_tokens)\n",
        "    print(tokenizer.decode(output[0]))\n",
        "    return tokenizer.decode(output[0] )\n"
      ],
      "metadata": {
        "id": "Ibt-JMIZK_As"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_for_completion(prompt, formalism_name):\n",
        "    system_message = f\"You are a model completion expert. You are specialized in {formalism_name}.\"\n",
        "    prompt_template=f'''<|im_start|>system\n",
        "    {system_message}<|im_end|>\n",
        "    <|im_start|>user\n",
        "    {prompt}<|im_end|>\n",
        "    <|im_start|>assistant\n",
        "    '''\n",
        "\n",
        "    generated = generate_text(tokenizer, model, prompt_template)\n",
        "    return generated"
      ],
      "metadata": {
        "id": "ukLA8D6WlW-_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extract new Elements-  Rule Based Extraction / Manual Parsing"
      ],
      "metadata": {
        "id": "A_RQ14lI2L7D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_response(generated):\n",
        "  # parsing reponse (Mistral)\n",
        "  parsed = generated.split(\"<|im_start|> assistant\\n\")[1]\n",
        "  if \"<|im_end|>\" in parsed:\n",
        "    parsed = parsed.split('<|im_end|>')[0]\n",
        "  return parsed"
      ],
      "metadata": {
        "id": "e4McLtQWJJHJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_json(parsed):\n",
        "  if \"```json\" in parsed:\n",
        "    parsed = parsed.split('```json')[1]\n",
        "  if \"```\" in parsed:\n",
        "    parsed = parsed.split('```')[0]\n",
        "  return parsed"
      ],
      "metadata": {
        "id": "vjKNzfcKgbt5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_elements_by_keys(data, keys):\n",
        "    # data: output generated by LLM (json file)\n",
        "    # keys: metamodel_elements\n",
        "    data = json.loads(data)\n",
        "    # Create a dictionary to store the results\n",
        "    results = {}\n",
        "\n",
        "    # Convert all dictionary keys to lowercase for case-insensitive comparison\n",
        "    data_lower = {k.lower(): v for k, v in data.items()}\n",
        "\n",
        "    # Iterate through each key provided and retrieve corresponding values\n",
        "    for key in keys:\n",
        "        # Convert the key to lowercase for case-insensitive comparison\n",
        "        key_lower = key.lower()\n",
        "        if key_lower in data_lower:\n",
        "            results[key] = data_lower[key_lower]\n",
        "        else:\n",
        "            results[key] = \"Key not found in data\"\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "7AoFyiJScN7i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Validate element by element if it is actually a good fit"
      ],
      "metadata": {
        "id": "fnAqoMNB2Vwj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_responses(feedback):\n",
        "    lines = feedback.split(\"\\n\")\n",
        "    responses = []\n",
        "    for line in lines:\n",
        "        if \"Sure\" in line:\n",
        "            index = lines.index(line)\n",
        "            responses.extend(lines[index+1:])\n",
        "            break\n",
        "    return \"\\n\".join(responses).split('</s>')[0]"
      ],
      "metadata": {
        "id": "kCO0pbYAWLQg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def validation_loop(elements, current_model, formalism_name, metamodel_elements):\n",
        "  feedback = []\n",
        "  for key, value in elements.items():\n",
        "    prompt_parts = [\n",
        "        f'I am developing a {formalism_name} which currently includes: {current_model}.',\n",
        "        f'An expert suggests adding {value} as {key} to the {formalism_name}.',\n",
        "        f'For each suggested elements, check if it is a good fit.',\n",
        "        f'Answer with only a simple Yes or No, following the element and :',\n",
        "        f'No additional explanations should be provided.',\n",
        "        f'Generate only the answer without any other text.'\n",
        "    ]\n",
        "    prompt = \" \".join(prompt_parts)\n",
        "    system_message = f\"You are a model completion expert. You are specialized in {formalism_name}.\"\n",
        "    prompt_template=f'''[INST] <<SYS>>\n",
        "{system_message}<</SYS>>\n",
        "{prompt}[/INST]'''\n",
        "\n",
        "\n",
        "    print('Output:\\n')\n",
        "    generated = generate_text(validation_tokenizer, validation_model, prompt_template, 200)\n",
        "    parsed = extract_responses(generated)\n",
        "    feedback.append(parsed)\n",
        "\n",
        "  return \"\\n\\n\".join(feedback)\n"
      ],
      "metadata": {
        "id": "o_UckRb6rB0A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Demo\n",
        "\n",
        "In the following, we give a running example of Class Diagram completion."
      ],
      "metadata": {
        "id": "V5fK2yCetVlu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "formalism_name = \"Class Diagrams (UML)\"\n",
        "current_model = \"Classes: Hospital, Doctor; Attributes: Hospital(name, address)\" # Users can customize current model\n",
        "metamodel_description = extract_metamodel_elements(formalism_name, ontology) # Extract metamodel elements from the JSON file containing modeling formalisms\n",
        "metamodel_description"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IAPHyckmnglX",
        "outputId": "487fe49b-4a0e-4439-f8a9-536b6575d5fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Classes', 'Attributes', 'Static Relationships']"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = generate_prompt_for_formalism(formalism_name,current_model,metamodel_description) # Prompt used for completion\n",
        "prompt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "id": "OiNYYtnzotm9",
        "outputId": "b2e1b760-831e-4f5b-c0b5-16070bcad59f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"I am developing a Class Diagrams (UML) which currently includes: Classes: Hospital, Doctor; Attributes: Hospital(name, address). Suggest new elements for each of the metamodel element in ['Classes', 'Attributes', 'Static Relationships'], that can complete the current model. Elements should be organized in a JSON format where key value is the name of the metamodel element. Provide only the JSON file. Do not provide extra explanations.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generated = generate_for_completion(prompt, formalism_name) # Model completion"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nzHFwF_po73x",
        "outputId": "ce93f071-f62e-46b1-f99e-7812e778cd34"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<s><|im_start|> system\n",
            "    You are a model completion expert. You are specialized in Class Diagrams (UML).<|im_end|> \n",
            "    <|im_start|> user\n",
            "    I am developing a Class Diagrams (UML) which currently includes: Classes: Hospital, Doctor; Attributes: Hospital(name, address). Suggest new elements for each of the metamodel element in ['Classes', 'Attributes', 'Static Relationships'], that can complete the current model. Elements should be organized in a JSON format where key value is the name of the metamodel element. Provide only the JSON file. Do not provide extra explanations.<|im_end|> \n",
            "    <|im_start|> assistant\n",
            "     {\n",
            "        \"Classes\": [\n",
            "            \"Patient\",\n",
            "            \"Appointment\",\n",
            "            \"MedicalRecord\"\n",
            "        ],\n",
            "        \"Attributes\": [\n",
            "            {\n",
            "                \"Hospital\": \"phoneNumber\"\n",
            "            },\n",
            "            {\n",
            "                \"Doctor\": \"specialty\"\n",
            "            },\n",
            "            {\n",
            "                \"Patient\": \"insurance\"\n",
            "            },\n",
            "            {\n",
            "                \"Appointment\": \"date\"\n",
            "            },\n",
            "            {\n",
            "                \"MedicalRecord\": \"diagnosis\"\n",
            "            }\n",
            "        ],\n",
            "        \"Static Relationships\": [\n",
            "            {\n",
            "                \"Hospital\": \"has\",\n",
            "                \"Doctors\": [\n",
            "                    \"Hospital\",\n",
            "                    \"Doctor\"\n",
            "                ]\n",
            "            },\n",
            "            {\n",
            "                \"Doctor\": \"treats\",\n",
            "                \"Patients\": [\n",
            "                    \"Doctor\",\n",
            "                    \"Patient\"\n",
            "                ]\n",
            "            },\n",
            "            {\n",
            "                \"Patient\": \"has\",\n",
            "                \"Appointments\": [\n",
            "                    \"Patient\",\n",
            "                    \"Appointment\"\n",
            "                ]\n",
            "            },\n",
            "            {\n",
            "                \"Appointment\": \"contains\",\n",
            "                \"MedicalRecords\": [\n",
            "                    \"Appointment\",\n",
            "                    \"MedicalRecord\"\n",
            "                ]\n",
            "            }\n",
            "        ]\n",
            "    }<|im_end|>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "parsed = parse_json(parse_response(generated)) # rule-based output extraction\n",
        "elements = get_elements_by_keys(parsed, metamodel_description) # rule-based element extraction\n",
        "elements"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7FM2o-GhsME3",
        "outputId": "be2d4f2c-93a0-4c2e-acde-97892dfc40ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Classes': ['Patient', 'Appointment', 'MedicalRecord'],\n",
              " 'Attributes': [{'Hospital': 'phoneNumber'},\n",
              "  {'Doctor': 'specialty'},\n",
              "  {'Patient': 'insurance'},\n",
              "  {'Appointment': 'date'},\n",
              "  {'MedicalRecord': 'diagnosis'}],\n",
              " 'Static Relationships': [{'Hospital': 'has',\n",
              "   'Doctors': ['Hospital', 'Doctor']},\n",
              "  {'Doctor': 'treats', 'Patients': ['Doctor', 'Patient']},\n",
              "  {'Patient': 'has', 'Appointments': ['Patient', 'Appointment']},\n",
              "  {'Appointment': 'contains',\n",
              "   'MedicalRecords': ['Appointment', 'MedicalRecord']}]}"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "model_name_or_path = \"TheBloke/Llama-2-7B-Chat-GPTQ\"\n",
        "# To use a different branch, change revision\n",
        "# For example: revision=\"gptq-4bit-32g-actorder_True\"\n",
        "validation_model = AutoModelForCausalLM.from_pretrained(model_name_or_path,\n",
        "                                              device_map=\"auto\",\n",
        "                                              trust_remote_code=False,\n",
        "                                              revision=\"main\")\n",
        "validation_tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Utn8MBIGl6LL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ef2785b-7761-41b4-e59f-052899370af2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:4371: FutureWarning: `_is_quantized_training_enabled` is going to be deprecated in transformers 4.39.0. Please use `model.hf_quantizer.is_trainable` instead\n",
            "  warnings.warn(\n",
            "The cos_cached attribute will be removed in 4.39. Bear in mind that its contents changed in v4.38. Use the forward method of RoPE from now on instead. It is not used in the `LlamaAttention` class\n",
            "The sin_cached attribute will be removed in 4.39. Bear in mind that its contents changed in v4.38. Use the forward method of RoPE from now on instead. It is not used in the `LlamaAttention` class\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "feedback = validation_loop(elements, current_model, formalism_name, metamodel_description) # validate metamodel elements respectively"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JFl9-a03sSvL",
        "outputId": "db8b7275-7a9d-4aab-dca5-518734bec8f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output:\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1256: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<s> [INST] <<SYS>>\n",
            "You are a model completion expert. You are specialized in Class Diagrams (UML).<</SYS>>\n",
            "I am developing a Class Diagrams (UML) which currently includes: Classes: Hospital, Doctor; Attributes: Hospital(name, address). An expert suggests adding ['Patient', 'Appointment', 'MedicalRecord'] as Classes to the Class Diagrams (UML). For each suggested elements, check if it is a good fit. Answer with only a simple Yes or No, following the element and : No additional explanations should be provided. Generate only the answer without any other text.[/INST]  Sure, I'd be happy to help you with that! Here are my answers to your suggested elements:\n",
            "* 'Patient': Yes\n",
            "* 'Appointment': Yes\n",
            "* 'MedicalRecord': Yes</s>\n",
            "Output:\n",
            "\n",
            "<s> [INST] <<SYS>>\n",
            "You are a model completion expert. You are specialized in Class Diagrams (UML).<</SYS>>\n",
            "I am developing a Class Diagrams (UML) which currently includes: Classes: Hospital, Doctor; Attributes: Hospital(name, address). An expert suggests adding [{'Hospital': 'phoneNumber'}, {'Doctor': 'specialty'}, {'Patient': 'insurance'}, {'Appointment': 'date'}, {'MedicalRecord': 'diagnosis'}] as Attributes to the Class Diagrams (UML). For each suggested elements, check if it is a good fit. Answer with only a simple Yes or No, following the element and : No additional explanations should be provided. Generate only the answer without any other text.[/INST]  Sure, I'd be happy to help! Here are my answers to your questions:\n",
            "1. Hospital : 'phoneNumber' - Yes\n",
            "2. Doctor : 'specialty' - Yes\n",
            "3. Patient : 'insurance' - No\n",
            "4. Appointment : 'date' - Yes\n",
            "5. MedicalRecord : 'diagnosis' - Yes</s>\n",
            "Output:\n",
            "\n",
            "<s> [INST] <<SYS>>\n",
            "You are a model completion expert. You are specialized in Class Diagrams (UML).<</SYS>>\n",
            "I am developing a Class Diagrams (UML) which currently includes: Classes: Hospital, Doctor; Attributes: Hospital(name, address). An expert suggests adding [{'Hospital': 'has', 'Doctors': ['Hospital', 'Doctor']}, {'Doctor': 'treats', 'Patients': ['Doctor', 'Patient']}, {'Patient': 'has', 'Appointments': ['Patient', 'Appointment']}, {'Appointment': 'contains', 'MedicalRecords': ['Appointment', 'MedicalRecord']}] as Static Relationships to the Class Diagrams (UML). For each suggested elements, check if it is a good fit. Answer with only a simple Yes or No, following the element and : No additional explanations should be provided. Generate only the answer without any other text.[/INST]  Sure, I'd be happy to help! Here are my answers to your questions:\n",
            "1. Hospital has 'Hospital': Yes\n",
            "2. Doctor treats 'Doctors': Yes\n",
            "3. Patient has 'Appointments': Yes\n",
            "4. Appointment contains 'MedicalRecords': Yes</s>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(feedback) # final extracted elements for completion"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sPWncmZbtNak",
        "outputId": "f637c927-b864-498b-fa61-ae98b98f85e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "* 'Patient': Yes\n",
            "* 'Appointment': Yes\n",
            "* 'MedicalRecord': Yes\n",
            "\n",
            "1. Hospital : 'phoneNumber' - Yes\n",
            "2. Doctor : 'specialty' - Yes\n",
            "3. Patient : 'insurance' - No\n",
            "4. Appointment : 'date' - Yes\n",
            "5. MedicalRecord : 'diagnosis' - Yes\n",
            "\n",
            "1. Hospital has 'Hospital': Yes\n",
            "2. Doctor treats 'Doctors': Yes\n",
            "3. Patient has 'Appointments': Yes\n",
            "4. Appointment contains 'MedicalRecords': Yes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "E5i6pNktlHZA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}